{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build-An-Agent\n",
        "In this notebook, we'll walk through the process of building an AI agent that integrates image generation, object detection, and language modeling. First, the agent will generate images from a text prompt using a pre-trained Stable Diffusion model. Then, it will analyze the generated image using a pre-trained object detection model to identify and label objects. Finally, an LLM will enhance the workflow by refining prompts and providing insightful analysis based on detected objects. This activity demonstrates how multiple machine learning pipelines can work together to create a powerful AI system for visual content creation and interpretation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "enlSi4z7Lvdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: You may have to change the runtime type if you encounter a run time error. Go to runtime --> change runtime type --> T4 GPU"
      ],
      "metadata": {
        "id": "dRVLjtq6Ywt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by downloading the libraries we need.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9t-DWwzXTHgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers torch matplotlib pillow openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Vrh-21SsXFaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers ctransformers"
      ],
      "metadata": {
        "id": "IyRk0bwAINer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's import the libraries we need."
      ],
      "metadata": {
        "id": "mtguThniTL3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import json\n",
        "import time\n",
        "from IPython.display import display\n"
      ],
      "metadata": {
        "id": "uLQyqUxVWClz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class handles image creation, enhancement, and analysis using a combination of Stable Diffusion and a language model for a more interactive and creative experience. Let's run it!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4g9SS4fKWV5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AIImageLLMAgent:\n",
        "    \"\"\"An AI agent for generating and analyzing images with LLM assistance\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sd_model=\"stabilityai/stable-diffusion-2-1-base\",\n",
        "                 output_dir=\"./generated_images\",\n",
        "                 device=None,\n",
        "                 llm_model=\"TheBloke/Llama-2-7B-Chat-GGML\",  # Open source LLM\n",
        "                 llm_enabled=True):\n",
        "        \"\"\"Initialize the AI Image Agent with models and configuration\"\"\"\n",
        "        self.output_dir = output_dir\n",
        "        self.llm_enabled = llm_enabled\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Determine device (GPU or CPU)\n",
        "        if device is None:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load the image generation model\n",
        "        print(\"Loading Stable Diffusion model...\")\n",
        "        torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "        self.sd_pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            sd_model,\n",
        "            torch_dtype=torch_dtype\n",
        "        )\n",
        "        self.sd_pipeline = self.sd_pipeline.to(self.device)\n",
        "\n",
        "        # Load the object detection model\n",
        "        print(\"Loading object detection model...\")\n",
        "        self.object_detector = pipeline(\"object-detection\")\n",
        "\n",
        "        # Initialize the LLM if enabled\n",
        "        self.llm = None\n",
        "        self.tokenizer = None\n",
        "        if llm_enabled:\n",
        "            try:\n",
        "                print(f\"Loading LLM model: {llm_model}\")\n",
        "                # Initialize the model - choose appropriate model type\n",
        "                if \"GGML\" in llm_model or \"ggml\" in llm_model:\n",
        "                    # For GGML quantized models, use CTransformers\n",
        "                    from ctransformers import AutoModelForCausalLM as CTAutoModelForCausalLM\n",
        "                    self.llm = CTAutoModelForCausalLM.from_pretrained(\n",
        "                        llm_model,\n",
        "                        model_type=\"llama\" if \"llama\" in llm_model.lower() else \"gpt2\"\n",
        "                    )\n",
        "                    self.tokenizer = AutoTokenizer.from_pretrained(llm_model)\n",
        "                else:\n",
        "                    # For regular HF models, use transformers\n",
        "                    self.tokenizer = AutoTokenizer.from_pretrained(llm_model)\n",
        "                    self.llm = AutoModelForCausalLM.from_pretrained(\n",
        "                        llm_model,\n",
        "                        torch_dtype=torch_dtype,\n",
        "                        low_cpu_mem_usage=True,\n",
        "                        device_map=\"auto\"\n",
        "                    )\n",
        "                print(\"Successfully loaded LLM model\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading LLM model: {e}\")\n",
        "                print(\"Continuing without LLM capabilities\")\n",
        "                self.llm_enabled = False\n",
        "        else:\n",
        "            print(\"LLM features disabled.\")\n",
        "\n",
        "    def generate_image(self, prompt, negative_prompt=\"\",\n",
        "                       num_inference_steps=50, guidance_scale=7.5,\n",
        "                       width=512, height=512, save=True):\n",
        "        \"\"\"Generate an image based on text prompt\"\"\"\n",
        "        print(f\"Generating image for prompt: '{prompt}'\")\n",
        "\n",
        "        # Generate the image\n",
        "        image = self.sd_pipeline(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            width=width,\n",
        "            height=height\n",
        "        ).images[0]\n",
        "\n",
        "        # Save the image if requested\n",
        "        if save:\n",
        "            # Create a filename from the prompt\n",
        "            filename = \"_\".join(prompt.split()[:5]).lower()\n",
        "            filename = \"\".join(c if c.isalnum() or c == \"_\" else \"_\" for c in filename)\n",
        "            filepath = os.path.join(self.output_dir, f\"{filename}.png\")\n",
        "            image.save(filepath)\n",
        "            print(f\"Image saved to {filepath}\")\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _generate_llm_response(self, system_prompt, user_prompt, max_length=1000):\n",
        "        \"\"\"Generate a response from the local LLM model\"\"\"\n",
        "        if not self.llm_enabled or self.llm is None or self.tokenizer is None:\n",
        "            return \"LLM functionality not available.\"\n",
        "\n",
        "        try:\n",
        "            # Format prompt based on Llama 2 chat template\n",
        "            # This format is for Llama 2 - adjust for other models\n",
        "            formatted_prompt = f\"\"\"<s>[INST] <<SYS>>\n",
        "{system_prompt}\n",
        "<</SYS>>\n",
        "\n",
        "{user_prompt} [/INST]\"\"\"\n",
        "\n",
        "            # Tokenize prompt\n",
        "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "            # Generate response\n",
        "            with torch.no_grad():\n",
        "                output = self.llm.generate(\n",
        "                    inputs[\"input_ids\"],\n",
        "                    max_new_tokens=max_length,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    repetition_penalty=1.1,\n",
        "                    do_sample=True\n",
        "                )\n",
        "\n",
        "            # Decode response\n",
        "            response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract only the assistant's response (after the prompt)\n",
        "            response = response.split(\"[/INST]\")[-1].strip()\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating LLM response: {e}\")\n",
        "            return \"Error generating response from LLM.\"\n",
        "\n",
        "    def enhance_prompt(self, user_prompt):\n",
        "        \"\"\"Use LLM to enhance a basic prompt into a more detailed one for better image generation\"\"\"\n",
        "        if not self.llm_enabled:\n",
        "            print(\"LLM enhancement not available: LLM features are disabled\")\n",
        "            return user_prompt\n",
        "\n",
        "        print(\"Enhancing prompt with local LLM...\")\n",
        "\n",
        "        system_message = \"\"\"\n",
        "        You are an expert at creating detailed prompts for AI image generation.\n",
        "        Take the user's basic prompt and enhance it with rich, descriptive details that will help\n",
        "        Stable Diffusion create a better image. Focus on visual elements, lighting, style, mood,\n",
        "        and composition. Return ONLY the enhanced prompt text with no explanation or additional content.\n",
        "        \"\"\"\n",
        "\n",
        "        enhanced_prompt = self._generate_llm_response(\n",
        "            system_message,\n",
        "            f\"Enhance this image prompt: {user_prompt}\"\n",
        "        )\n",
        "\n",
        "        # Clean up the response - remove any explanations the LLM might add\n",
        "        if \":\" in enhanced_prompt:\n",
        "            enhanced_prompt = enhanced_prompt.split(\":\")[-1].strip()\n",
        "\n",
        "        print(f\"Original prompt: {user_prompt}\")\n",
        "        print(f\"Enhanced prompt: {enhanced_prompt}\")\n",
        "        return enhanced_prompt\n",
        "\n",
        "    def analyze_image_with_llm(self, image, detections):\n",
        "        \"\"\"Use LLM to provide an insightful analysis of the image and detected objects\"\"\"\n",
        "        if not self.llm_enabled:\n",
        "            print(\"LLM analysis not available: LLM features are disabled\")\n",
        "            return None\n",
        "\n",
        "        print(\"Analyzing image with local LLM...\")\n",
        "\n",
        "        # Prepare detection data for LLM\n",
        "        detection_text = \"Objects detected:\\n\"\n",
        "        if detections:\n",
        "            for i, detection in enumerate(detections):\n",
        "                detection_text += f\"{i+1}. {detection['label']} (confidence: {detection['score']:.2f})\\n\"\n",
        "        else:\n",
        "            detection_text += \"No objects detected by the object detection model.\\n\"\n",
        "\n",
        "        system_message = \"\"\"\n",
        "        You are an expert art critic and image analyst. Provide a thoughtful, insightful analysis of the image\n",
        "        based on the detected objects and what you can infer about the composition, style, and content.\n",
        "        Consider both what the object detection model found and what might be missing or misinterpreted.\n",
        "        Keep your analysis to 3-4 paragraphs of insightful observations.\n",
        "        \"\"\"\n",
        "\n",
        "        analysis = self._generate_llm_response(\n",
        "            system_message,\n",
        "            f\"Here is the object detection result:\\n\\n{detection_text}\\n\\nPlease provide your analysis of this image.\"\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- LLM's Analysis ---\")\n",
        "        print(analysis)\n",
        "        return analysis\n",
        "\n",
        "    def detect_objects(self, image, threshold=0.3, visualize=True, llm_analyze=False):\n",
        "        \"\"\"Detect objects in the image\"\"\"\n",
        "        print(\"Analyzing image for objects...\")\n",
        "\n",
        "        # Perform object detection\n",
        "        results = self.object_detector(image, threshold=threshold)\n",
        "\n",
        "        # Print detection results\n",
        "        if results:\n",
        "            print(f\"Detected {len(results)} objects:\")\n",
        "            for detection in results:\n",
        "                print(f\"- {detection['label']} ({detection['score']:.2f})\")\n",
        "        else:\n",
        "            print(\"No objects detected.\")\n",
        "\n",
        "        # Get LLM analysis if requested\n",
        "        if llm_analyze and self.llm_enabled:\n",
        "            self.analyze_image_with_llm(image, results)\n",
        "\n",
        "        # Visualize the results if requested\n",
        "        if visualize:\n",
        "            return self._visualize_detections(image, results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _visualize_detections(self, image, detections):\n",
        "        \"\"\"Visualize detected objects on the image\"\"\"\n",
        "        # Create a copy of the image to draw on\n",
        "        image_with_boxes = image.copy()\n",
        "        draw = ImageDraw.Draw(image_with_boxes)\n",
        "\n",
        "        # Try to get a font\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"arial.ttf\", 15)\n",
        "        except IOError:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "        # Draw boxes and labels for each detection\n",
        "        for detection in detections:\n",
        "            score = detection[\"score\"]\n",
        "            label = detection[\"label\"]\n",
        "            box = detection[\"box\"]\n",
        "\n",
        "            left = box['xmin']\n",
        "            top = box['ymin']\n",
        "            right = box['xmax']\n",
        "            bottom = box['ymax']\n",
        "\n",
        "            # Draw bounding box\n",
        "            draw.rectangle([left, top, right, bottom], outline=\"red\", width=3)\n",
        "\n",
        "            # Draw label with score\n",
        "            text = f\"{label}: {score:.2f}\"\n",
        "            text_width, text_height = draw.textsize(text, font=font) if hasattr(draw, 'textsize') else (len(text) * 7, 15)\n",
        "            draw.rectangle([left, top, left + text_width, top + text_height], fill=\"red\")\n",
        "            draw.text((left, top), text, fill=\"white\", font=font)\n",
        "\n",
        "        return image_with_boxes\n",
        "\n",
        "    def get_creative_image_ideas(self, topic):\n",
        "        \"\"\"Generate creative image prompt ideas based on a topic using LLM\"\"\"\n",
        "        if not self.llm_enabled:\n",
        "            print(\"Feature not available: LLM features are disabled\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Generating creative image ideas for topic: '{topic}'...\")\n",
        "\n",
        "        system_message = \"\"\"\n",
        "        You are a creative director specializing in visual concepts.\n",
        "        Generate 5 unique, interesting image prompt ideas related to the user's topic.\n",
        "        Each prompt should be detailed and visually descriptive, ready to use with Stable Diffusion.\n",
        "        Format your response as a numbered list with one prompt per line, starting each line with a number and a period.\n",
        "        Example:\n",
        "        1. First prompt idea\n",
        "        2. Second prompt idea\n",
        "        3. Third prompt idea\n",
        "        4. Fourth prompt idea\n",
        "        5. Fifth prompt idea\n",
        "        \"\"\"\n",
        "\n",
        "        response_text = self._generate_llm_response(\n",
        "            system_message,\n",
        "            f\"Generate 5 creative and unique image prompt ideas related to: {topic}.\"\n",
        "        )\n",
        "\n",
        "        # Parse the response into a list of ideas\n",
        "        ideas = []\n",
        "        for line in response_text.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if line and (line[0].isdigit() or line.startswith(\"- \")):\n",
        "                # Remove the number/bullet and get just the prompt\n",
        "                prompt = line.split('.', 1)[-1].strip() if '.' in line else line[2:].strip()\n",
        "                if prompt:\n",
        "                    ideas.append(prompt)\n",
        "\n",
        "        # If parsing failed, just split by newlines and take up to 5 items\n",
        "        if not ideas:\n",
        "            ideas = [line.strip() for line in response_text.split('\\n') if line.strip()][:5]\n",
        "\n",
        "        print(\"\\n--- Creative Image Ideas ---\")\n",
        "        for i, idea in enumerate(ideas):\n",
        "            print(f\"{i+1}. {idea}\")\n",
        "\n",
        "        return ideas\n",
        "\n",
        "    def interactive_session(self):\n",
        "        \"\"\"Start an interactive session with the agent\"\"\"\n",
        "        print(\"\\n===== AI Image LLM Agent Interactive Session =====\")\n",
        "        print(\"Type 'exit' or 'quit' to end the session.\\n\")\n",
        "\n",
        "        current_image = None\n",
        "        last_filename = None\n",
        "\n",
        "        while True:\n",
        "            print(\"\\nAvailable commands:\")\n",
        "            print(\"1. generate - Generate an image from a text prompt\")\n",
        "            print(\"2. enhance - Use LLM to enhance a basic prompt\")\n",
        "            print(\"3. analyze - Detect and analyze objects in an image\")\n",
        "            print(\"4. ideas - Get creative image prompt ideas\")\n",
        "            print(\"5. help - Show detailed help\")\n",
        "            print(\"6. quit - Exit the session\")\n",
        "\n",
        "            command = input(\"\\nEnter command (1-6): \").strip().lower()\n",
        "\n",
        "            if command in ['quit', 'exit', '6']:\n",
        "                print(\"Exiting interactive session.\")\n",
        "                break\n",
        "\n",
        "            elif command in ['help', '5']:\n",
        "                self._print_help()\n",
        "\n",
        "            elif command in ['generate', '1']:\n",
        "                prompt = input(\"Enter image prompt: \")\n",
        "                negative_prompt = input(\"Enter negative prompt (optional): \")\n",
        "\n",
        "                current_image = self.generate_image(prompt, negative_prompt=negative_prompt)\n",
        "                self._display_image(current_image)\n",
        "\n",
        "            elif command in ['enhance', '2']:\n",
        "                if not self.llm_enabled:\n",
        "                    print(\"Feature not available: LLM features are disabled\")\n",
        "                    continue\n",
        "\n",
        "                basic_prompt = input(\"Enter basic image concept: \")\n",
        "                enhanced_prompt = self.enhance_prompt(basic_prompt)\n",
        "\n",
        "                use_enhanced = input(\"Generate image with this enhanced prompt? (y/n): \").lower()\n",
        "                if use_enhanced == 'y':\n",
        "                    current_image = self.generate_image(enhanced_prompt)\n",
        "                    self._display_image(current_image)\n",
        "\n",
        "            elif command in ['analyze', '3']:\n",
        "                if current_image is None:\n",
        "                    filepath = input(\"Enter path to image (or press Enter to load the last generated): \")\n",
        "                    if filepath:\n",
        "                        try:\n",
        "                            current_image = Image.open(filepath)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error loading image: {e}\")\n",
        "                            continue\n",
        "                    elif last_filename:\n",
        "                        try:\n",
        "                            current_image = Image.open(last_filename)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error loading last image: {e}\")\n",
        "                            continue\n",
        "                    else:\n",
        "                        print(\"No image to analyze. Generate or load an image first.\")\n",
        "                        continue\n",
        "\n",
        "                threshold = input(\"Detection threshold (0.0-1.0, default 0.3): \")\n",
        "                threshold = float(threshold) if threshold else 0.3\n",
        "\n",
        "                llm_analyze = input(\"Use LLM for in-depth analysis? (y/n, default n): \").lower() == 'y'\n",
        "\n",
        "                image_with_detections = self.detect_objects(current_image, threshold=threshold, llm_analyze=llm_analyze)\n",
        "                self._display_image(image_with_detections)\n",
        "\n",
        "            elif command in ['ideas', '4']:\n",
        "                if not self.llm_enabled:\n",
        "                    print(\"Feature not available: LLM features are disabled\")\n",
        "                    continue\n",
        "\n",
        "                topic = input(\"Enter a topic or theme for image ideas: \")\n",
        "                ideas = self.get_creative_image_ideas(topic)\n",
        "\n",
        "                if ideas:\n",
        "                    selection = input(\"Enter idea number to generate (or 0 to skip): \")\n",
        "                    try:\n",
        "                        idx = int(selection) - 1\n",
        "                        if 0 <= idx < len(ideas):\n",
        "                            current_image = self.generate_image(ideas[idx])\n",
        "                            self._display_image(current_image)\n",
        "                    except:\n",
        "                        print(\"Invalid selection, not generating an image.\")\n",
        "\n",
        "            else:\n",
        "                print(\"Unknown command. Type 'help' to see available commands.\")\n",
        "\n",
        "    def _print_help(self):\n",
        "        \"\"\"Print help information for interactive mode\"\"\"\n",
        "        print(\"\\n--- Command Details ---\")\n",
        "        print(\"1. generate - Generate a new image from a text prompt\")\n",
        "        print(\"   - Enter a detailed text description of the image you want to create\")\n",
        "        print(\"   - Optionally provide a negative prompt (things to avoid in the image)\")\n",
        "\n",
        "        print(\"\\n2. enhance - Use LLM to enhance a basic prompt\")\n",
        "        print(\"   - Enter a simple concept and LLM will expand it with visual details\")\n",
        "        print(\"   - Great for getting better results from Stable Diffusion\")\n",
        "\n",
        "        print(\"\\n3. analyze - Detect objects in the current or specified image\")\n",
        "        print(\"   - Uses computer vision to identify objects in the image\")\n",
        "        print(\"   - Can optionally use LLM to provide an in-depth analysis\")\n",
        "\n",
        "        print(\"\\n4. ideas - Get creative image prompt ideas on a topic\")\n",
        "        print(\"   - Enter a general topic and LLM will suggest specific image concepts\")\n",
        "        print(\"   - You can select one of the ideas to generate immediately\")\n",
        "\n",
        "        print(\"\\n5. help - Show this help message\")\n",
        "        print(\"\\n6. quit/exit - End the interactive session\")\n",
        "\n",
        "    def _display_image(self, image):\n",
        "        \"\"\"Display an image using matplotlib\"\"\"\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(np.array(image))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the AI Image LLM Agent\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"AI Image Generator and Analyzer with LLM\")\n",
        "    parser.add_argument(\"--sd_model\", default=\"stabilityai/stable-diffusion-2-1-base\",\n",
        "                        help=\"Stable Diffusion model to use\")\n",
        "    parser.add_argument(\"--output_dir\", default=\"./generated_images\",\n",
        "                        help=\"Directory to save generated images\")\n",
        "    parser.add_argument(\"--interactive\", action=\"store_true\",\n",
        "                        help=\"Start in interactive mode\")\n",
        "    parser.add_argument(\"--prompt\",\n",
        "                        help=\"Text prompt for image generation (non-interactive mode)\")\n",
        "    parser.add_argument(\"--device\", choices=[\"cuda\", \"cpu\"],\n",
        "                        help=\"Device to use (default: auto-detect)\")\n",
        "    parser.add_argument(\"--llm_model\", default=\"TheBloke/Llama-2-7B-Chat-GGML\",\n",
        "                        help=\"LLM model to use for text generation\")\n",
        "    parser.add_argument(\"--llm_disabled\", action=\"store_true\",\n",
        "                        help=\"Disable LLM features\")\n",
        "    parser.add_argument(\"--enhance\", action=\"store_true\",\n",
        "                        help=\"Use LLM to enhance the prompt\")\n",
        "    parser.add_argument(\"--analyze\", action=\"store_true\",\n",
        "                        help=\"Analyze the generated image with LLM\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Create the agent\n",
        "    agent = AIImageLLMAgent(\n",
        "        sd_model=args.sd_model,\n",
        "        output_dir=args.output_dir,\n",
        "        device=args.device,\n",
        "        llm_model=args.llm_model,\n",
        "        llm_enabled=not args.llm_disabled\n",
        "    )\n",
        "\n",
        "    # Run in interactive mode or generate a single image\n",
        "    if args.interactive:\n",
        "        agent.interactive_session()\n",
        "    elif args.prompt:\n",
        "        # Enhance the prompt if requested and possible\n",
        "        prompt = args.prompt\n",
        "        if args.enhance and not args.llm_disabled:\n",
        "            prompt = agent.enhance_prompt(prompt)\n",
        "\n",
        "        # Generate the image\n",
        "        image = agent.generate_image(prompt)\n",
        "\n",
        "        # Analyze if requested\n",
        "        if args.analyze and not args.llm_disabled:\n",
        "            detections = agent.detect_objects(image, llm_analyze=True)\n",
        "        else:\n",
        "            detections = agent.detect_objects(image)\n",
        "\n",
        "        # Display the image with detections\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(np.array(detections))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        parser.print_help()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "4Z3jZiCSGiyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start an interactive session where the user can generate images, improve prompts, and analyze results, interacting with both the image model and language model."
      ],
      "metadata": {
        "id": "XYC2Db_iau4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = AIImageLLMAgent(\n",
        "    sd_model=\"stabilityai/stable-diffusion-2-1-base\",\n",
        "    output_dir=\"./generated_images\",\n",
        "    llm_model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # Smaller model\n",
        "    llm_enabled=True\n",
        ")\n",
        "agent.interactive_session()"
      ],
      "metadata": {
        "id": "twNNWEoxIDkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You've created your first AI agent. Feel free to keep exploring its capabilities and challenge it by adjusting your prompts and confidence levels. There's a lot to discover, so dive in and see how it can adapt and respond!\n"
      ],
      "metadata": {
        "id": "KeaEO34ebCH8"
      }
    }
  ]
}